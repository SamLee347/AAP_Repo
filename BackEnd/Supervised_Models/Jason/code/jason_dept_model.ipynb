{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc537b64",
   "metadata": {},
   "source": [
    "# AAP AI Model for Sample Categorization - CLEAN VERSION\n",
    "\n",
    "**Admin Number**: 230327F  \n",
    "**Module**: IT3100 - AI Applications Project  \n",
    "**Jason Hong Jie Sen**\n",
    "\n",
    "---\n",
    "\n",
    "This is a clean version of the notebook without Git merge conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd6dfc3",
   "metadata": {},
   "source": [
    "## Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb4f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(r'C:\\Users\\jason\\OneDrive\\Documents\\2025 Sem1 Study\\IT3100_AAP\\AAP_Dataset (Supply Chain)\\Train_Set.csv')\n",
    "test_df = pd.read_csv(r'C:\\Users\\jason\\OneDrive\\Documents\\2025 Sem1 Study\\IT3100_AAP\\AAP_Dataset (Supply Chain)\\Test_Set.csv')\n",
    "\n",
    "# Sample data for faster processing\n",
    "train_df = train_df.sample(n=3000, random_state=42)\n",
    "test_df = test_df.sample(n=1000, random_state=42)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_df = train_df.drop(['OrderId', 'Customer_Id', 'Dept_Id', 'Zipcode', 'Prod_Category_Id', 'CategoryName'], axis=1)\n",
    "test_df = test_df.drop(['OrderId', 'Customer_Id', 'Dept_Id', 'Zipcode', 'Prod_Category_Id', 'CategoryName'], axis=1)\n",
    "\n",
    "# Clean columns\n",
    "train_df.columns = train_df.columns.str.strip()\n",
    "test_df.columns = test_df.columns.str.strip()\n",
    "\n",
    "# Handle missing values and duplicates\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "test_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Standardize string columns\n",
    "str_cols = ['Product_type', 'Customer_Category', 'Dept_Name', 'Shipping_Class', 'Warehouse_Region', 'Order_zone']\n",
    "for col in str_cols:\n",
    "    train_df[col] = train_df[col].str.strip().str.lower()\n",
    "    test_df[col] = test_df[col].str.strip().str.lower()\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af507ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Dept_Name_encoded'] = label_encoder.fit_transform(train_df['Dept_Name'])\n",
    "\n",
    "# Handle low-support classes\n",
    "low_support_classes = [1, 2, 4, 7, 9, 10]\n",
    "train_df['Dept_Name_encoded'] = train_df['Dept_Name_encoded'].apply(lambda x: 'Other' if x in low_support_classes else x)\n",
    "train_df['Dept_Name_encoded'] = train_df['Dept_Name_encoded'].astype(str)\n",
    "train_df['Dept_Name_encoded'] = label_encoder.fit_transform(train_df['Dept_Name_encoded'])\n",
    "\n",
    "# Select features\n",
    "features_to_keep = ['Price', 'Sales', 'Order_Profit', 'ProductWeight', 'Quantity']\n",
    "X = train_df[features_to_keep]\n",
    "y = train_df['Dept_Name_encoded']\n",
    "X_test = test_df[features_to_keep]\n",
    "\n",
    "print(\"Feature selection completed!\")\n",
    "print(f\"Features: {features_to_keep}\")\n",
    "print(f\"Target classes: {len(set(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE for class balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train Gradient Boosting model with hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Model training completed!\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_val_pred = grid_search.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae4dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "print(\"\\n=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "if hasattr(grid_search, 'best_estimator_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': grid_search.best_estimator_.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance Rankings:\")\n",
    "    print(feature_importance.to_string())\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Model not trained yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "test_predictions = grid_search.predict(X_test)\n",
    "test_predictions_decoded = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Create submission dataframe\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Predicted_Dept_Name': test_predictions_decoded\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "predictions_df.to_csv('predicted_dept_names.csv', index=False)\n",
    "print(\"Predictions saved to 'predicted_dept_names.csv'\")\n",
    "print(f\"Total predictions: {len(predictions_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and label encoder\n",
    "joblib.dump(grid_search.best_estimator_, '../model/gradient_boosting_model.pkl')\n",
    "\n",
    "with open('../model/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"Model and label encoder saved successfully!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- ../model/gradient_boosting_model.pkl\")\n",
    "print(\"- ../model/label_encoder.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
