{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf1a1fe-2e4d-4bdb-8506-cbcf46dfd128",
   "metadata": {},
   "source": [
    "# AAP AI Model for Sample Categorization\n",
    "\n",
    "**Admin Number**: 230327F  \n",
    "**Module**: IT3100 - AI Applications Project  \n",
    "**Jason Hong Jie Sen**\n",
    "\n",
    "---\n",
    "\n",
    "## **Progress Review 1 Summary**\n",
    "\n",
    "### **What Has Been Done in Progress Review 1:**\n",
    "1. **Data Preprocessing**:\n",
    "   - Handled missing values and removed duplicates.\n",
    "   - Standardized and encoded categorical features using **one-hot encoding**.\n",
    "   - Outliers were removed using **percentile trimming** (1st and 99th percentiles).\n",
    "   \n",
    "2. **Model Training**:\n",
    "   - Trained a **Random Forest Classifier** using preprocessed data.\n",
    "   - Evaluated the model using **accuracy**, **precision**, **recall**, and **F1-score**.\n",
    "\n",
    "3. **Next Steps**:\n",
    "   - **Hyperparameter tuning** for model optimization.\n",
    "   - **Class balancing** techniques (like **SMOTE** or **class weights**).\n",
    "   - **Feature engineering** to improve model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction**\n",
    "\n",
    "### Problem Statement:\n",
    "\n",
    "At ANS, the manual categorization of product samples is slow, inconsistent, and error-prone. This project aims to automate the classification of product samples based on metadata to improve **organization**, **efficiency**, and **accuracy**.\n",
    "\n",
    "### Goal:\n",
    "\n",
    "Build a supervised machine learning model to predict the **product category** using historical inventory and transaction data.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Data Collection and Justification**\n",
    "\n",
    "### Dataset Source:\n",
    "The dataset used is the **Supply Chain Management Dataset** on Kaggle:  \n",
    "[Supply Chain Management Dataset on Kaggle](https://www.kaggle.com/datasets/codymiles/supply-chain-management).\n",
    "\n",
    "### Dataset Features:\n",
    "- **Order-Related Info**: Order ID, customer details, sales data, order zone, price, category, etc.\n",
    "- **Product Info**: Product type, category, dimensions, weight, dispatch status.\n",
    "- **Shipping Info**: Shipping class, scheduled shipping, warehouse region, etc.\n",
    "\n",
    "### Relevance to ANS:\n",
    "This dataset provides the necessary features for building a model to categorize samples, aligning with ANSâ€™s operations focused on product distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Data Preparation and Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daf4d227-fb64-453d-8ff1-5f45004a7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(r'C:\\Users\\jason\\OneDrive\\Documents\\2025 Sem1 Study\\IT3100_AAP\\AAP_Dataset (Supply Chain)\\Train_Set.csv')\n",
    "test_df = pd.read_csv(r'C:\\Users\\jason\\OneDrive\\Documents\\2025 Sem1 Study\\IT3100_AAP\\AAP_Dataset (Supply Chain)\\Test_Set.csv')\n",
    "\n",
    "# TO REMOVEEEEEEEEEE (3000 rows) so it's faster:\n",
    "train_df = train_df.sample(n=3000, random_state=42)\n",
    "test_df = test_df.sample(n=1000, random_state=42)\n",
    "\n",
    "# Drop unnecessary identifier columns and other less useful features\n",
    "train_df = train_df.drop(['OrderId', 'Customer_Id', 'Dept_Id', 'Zipcode', 'Prod_Category_Id', 'CategoryName'], axis=1)\n",
    "test_df = test_df.drop(['OrderId', 'Customer_Id', 'Dept_Id', 'Zipcode', 'Prod_Category_Id', 'CategoryName'], axis=1)\n",
    "\n",
    "# Clean columns by stripping whitespace\n",
    "train_df.columns = train_df.columns.str.strip()\n",
    "test_df.columns = test_df.columns.str.strip()\n",
    "\n",
    "# Drop rows with missing values and duplicates\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "test_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Standardize string columns by converting to lowercase and stripping whitespace\n",
    "str_cols = ['Product_type', 'Customer_Category', 'Dept_Name', 'Shipping_Class', 'Warehouse_Region', 'Order_zone']\n",
    "for col in str_cols:\n",
    "    train_df[col] = train_df[col].str.strip().str.lower()\n",
    "    test_df[col] = test_df[col].str.strip().str.lower()\n",
    "\n",
    "# First encode all classes to identify low-support ones\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Dept_Name_encoded'] = label_encoder.fit_transform(train_df['Dept_Name'])\n",
    "\n",
    "# List low-support classes to combine into \"Other\"\n",
    "low_support_classes = [1, 2, 4, 7, 9, 10]\n",
    "\n",
    "# Replace low-support classes with a common \"Other\" category (make sure \"Other\" is a string)\n",
    "train_df['Dept_Name_encoded'] = train_df['Dept_Name_encoded'].apply(lambda x: 'Other' if x in low_support_classes else x)\n",
    "\n",
    "# Convert all values to string (to make sure the LabelEncoder works)\n",
    "train_df['Dept_Name_encoded'] = train_df['Dept_Name_encoded'].astype(str)\n",
    "\n",
    "# Now, apply LabelEncoder to encode the target variable after the replacement\n",
    "train_df['Dept_Name_encoded'] = label_encoder.fit_transform(train_df['Dept_Name_encoded'])\n",
    "\n",
    "# List of features to keep (based on feature importance)\n",
    "features_to_keep = ['Price', 'Sales', 'Order_Profit', 'ProductWeight', 'Quantity']\n",
    "\n",
    "# Preparing X and y with the selected features\n",
    "X = train_df[features_to_keep]\n",
    "y = train_df['Dept_Name_encoded']\n",
    "\n",
    "X_test = test_df[features_to_keep]  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0c31d-bd71-43fe-97bd-72a029bbd985",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. Loaded and cleaned the training and test datasets\n",
    "2. Standardized column names by stripping whitespace\n",
    "3. Removed rows with missing values and duplicate entries\n",
    "4. Normalized string columns by converting to lowercase and stripping whitespace\n",
    "5. Encoded the target variable (`Dept_Name`) using LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d0205a6-3f82-4758-aecd-0e0c0da04666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE for oversampling with fewer neighbors\n",
    "smote = SMOTE(random_state=42, k_neighbors=3)  \n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db1a7ee-1bf3-4757-956f-ad260a1fe4c3",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "1. Separated features (X) and target (y)\n",
    "2. Performed one-hot encoding on categorical columns\n",
    "3. Split data into training and validation sets (80/20)\n",
    "4. Addressed class imbalance using SMOTE oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc2560-9e7c-4b85-b341-a73785bd9930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]  # Make sure this is valid\n",
    "}\n",
    "\n",
    "\n",
    "# Re-run GridSearchCV with the updated grid\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d88c01-8a1f-4742-872a-7b6400702781",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Performed grid search over key parameters:\n",
    "- Number of estimators (50, 100, 150)\n",
    "- Tree depth (10, 20, unlimited)\n",
    "- Minimum samples for splits (2, 5, 10)\n",
    "- Minimum samples per leaf (1, 2, 4)\n",
    "\n",
    "Used 3-fold CV for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca887f-4f6d-420c-a5d6-f6a7a88dcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature Importance Analysis ===\n",
    "print(\"\\n=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "# Ensure you have a trained model (grid_search should be fitted)\n",
    "if hasattr(grid_search, 'best_estimator_'):\n",
    "    # Get feature importances from the best model\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': grid_search.best_estimator_.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Print the Feature Importance Rankings\n",
    "    print(\"\\nFeature Importance Rankings:\")\n",
    "    print(feature_importance.to_string())  # to_string() for better formatting\n",
    "    \n",
    "    # Plot the Top 10 Most Important Features\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance.head(10), \n",
    "                x='importance', \n",
    "                y='feature', \n",
    "                palette='viridis')\n",
    "    plt.title('Top 10 Most Important Features')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display Top 5 Features with their importance\n",
    "    print(\"\\nTop 5 most important features:\")\n",
    "    top_features = feature_importance.head(5)\n",
    "    for i, (idx, row) in enumerate(top_features.iterrows()):\n",
    "        print(f\"{i+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Show cumulative importance\n",
    "    print(f\"\\nThese top 5 features account for {top_features['importance'].sum():.1%} of the model's decision-making\")\n",
    "    \n",
    "    # Remove low-importance features\n",
    "    threshold = 0.01  # Adjust based on your feature importance distribution\n",
    "    important_features = feature_importance[feature_importance['importance'] > threshold]['feature']\n",
    "    \n",
    "    print(f\"\\nKeeping {len(important_features)} features with importance > {threshold}\")\n",
    "    X = X[important_features]  # Filter features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367f327-6c37-4009-ac1b-1a99e6969f72",
   "metadata": {},
   "source": [
    "# Remove features with very low importance (below a certain threshold)\n",
    "threshold = 0.01  # You can adjust this threshold based on the distribution of importance values\n",
    "important_features = feature_importance[feature_importance['importance'] > threshold]\n",
    "X = X[important_features['feature']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac029f-4189-484f-9a88-37499c3adbca",
   "metadata": {},
   "source": [
    "## Correlation Matrix & Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf9b4f-f7f3-4786-a3b3-893b27cd0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train Gradient Boosting model \n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions using RandomForest's best model (grid_search)\n",
    "y_test_pred = grid_search.best_estimator_.predict(X_test)  # Now X_test exists\n",
    "\n",
    "# Convert predictions to department names\n",
    "predicted_dept = label_encoder.inverse_transform(y_test_pred)\n",
    "\n",
    "# Save predictions\n",
    "test_df['Predicted_Dept_Name'] = predicted_dept\n",
    "test_df.to_csv(\"predicted_dept_names.csv\", index=False)\n",
    "\n",
    "# Validation (using GradientBoosting - if you want)\n",
    "y_pred_gb = gb_model.predict(X_val)  # Need X_val prepared similarly\n",
    "y_val_dept = label_encoder.inverse_transform(y_val)\n",
    "y_pred_dept = label_encoder.inverse_transform(y_pred_gb)\n",
    "\n",
    "print(\"GB Classification Report:\\n\", classification_report(y_val_dept, y_pred_dept))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val_dept, y_pred_dept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e318112-a8a4-4cb6-86c3-1117523a52a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming your model is trained (e.g., gb_model)\n",
    "# Save the model to a specific path\n",
    "file_path = r'C:\\Users\\jason\\OneDrive\\Documents\\2025 Sem1 Study\\IT3100_AAP\\final pkl file\\gradient_boosting_model.pkl'\n",
    "joblib.dump(gb_model, file_path)\n",
    "\n",
    "# Save the label encoder as well (if needed)\n",
    "encoder_path = r'C:\\Users\\jason\\OneDrive\\Documents\\2025 Sem1 Study\\IT3100_AAP\\final pkl file\\label_encoder.pkl'\n",
    "joblib.dump(label_encoder, encoder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca1d68-0180-469d-8654-8066c6c217dd",
   "metadata": {},
   "source": [
    "## Model Initialization and Training\n",
    "\n",
    "Initialized gradient boosting model after comparing with RandomForest\n",
    "\n",
    "Performed 3-fold cross-validation to assess generalization performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05dc444-4542-441b-b201-59820ede7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input():\n",
    "    print(\"Enter the following information:\")\n",
    "\n",
    "    # Collecting user input data\n",
    "    price = float(input(\"Price (in currency): \"))\n",
    "    sales = float(input(\"Sales: \"))\n",
    "    order_profit = float(input(\"Order Profit: \"))\n",
    "    quantity = float(input(\"Quantity: \"))\n",
    "    product_weight = float(input(\"Product Weight (in grams): \"))\n",
    "    customer_category = input(\"Customer Category (e.g. 'consumer', 'corporate', 'others'): \").lower()\n",
    "    cust_state = input(\"Customer State (e.g. 'uk', 'us', 'ca'): \").lower()\n",
    "    product_type = input(\"Product Type (e.g. 'fragile', 'not fragile'): \").lower()\n",
    "    dispatched = input(\"Dispatched (e.g. '1' or '0'): \").lower()\n",
    "    shipping_class = input(\"Shipping Class (e.g. 'standard', 'elite'): \").lower()\n",
    "    scheduled_shipping = int(input(\"Scheduled Shipping (1-7 scale): \"))\n",
    "    warehouse_region = input(\"Warehouse Region (e.g. 'north', 'south', etc.): \").lower()\n",
    "    delivery_review = input(\"Delivery Review (1-5 scale): \").lower()\n",
    "    weekday_order = int(input(\"Weekday Order (0=Sunday, 1=Monday,...,6=Saturday): \"))\n",
    "    delivery_status = input(\"Delivery Status (0 or 1): \").lower()\n",
    "\n",
    "    # Creating the dictionary of user inputs\n",
    "    user_data = {\n",
    "        'Price': price,\n",
    "        'Sales': sales,\n",
    "        'Order_Profit': order_profit,\n",
    "        'Quantity': quantity,\n",
    "        'ProductWeight': product_weight,\n",
    "        'Customer_Category': customer_category,\n",
    "        'Cust_State': cust_state,\n",
    "        'Product_type': product_type,\n",
    "        'Dispatched': dispatched,\n",
    "        'Shipping_Class': shipping_class,\n",
    "        'Scheduled_Shipping': scheduled_shipping,\n",
    "        'Warehouse_Region': warehouse_region,\n",
    "        'Delivery_Review': delivery_review,\n",
    "        'WeekdayOrder': weekday_order,\n",
    "        'Delivery_Status': delivery_status\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    user_df = pd.DataFrame([user_data])\n",
    "\n",
    "    # One-hot encoding of categorical features\n",
    "    user_df = pd.get_dummies(user_df, columns=['Customer_Category', 'Product_type', 'Shipping_Class', 'Warehouse_Region', 'Cust_State', 'Delivery_Review', 'Delivery_Status'], drop_first=True)\n",
    "\n",
    "    # Make sure user input has the same columns as the training data\n",
    "    user_df = user_df.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "    # Load the trained model\n",
    "    model = joblib.load('gradient_boosting_model.pkl')\n",
    "\n",
    "    # Get the prediction (numeric value)\n",
    "    prediction = model.predict(user_df)\n",
    "\n",
    "    # Decode the predicted department (numeric to string)\n",
    "    predicted_dept = label_encoder.inverse_transform(prediction)\n",
    "\n",
    "    # Print the department name\n",
    "    print(f\"Predicted Department: {predicted_dept[0]}\")\n",
    "\n",
    "# Call the user input function\n",
    "user_input()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd596b-0a51-4da6-8cf0-f6c6cc07fe35",
   "metadata": {},
   "source": [
    "## User input (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09238125-1fa2-48b6-9474-74e665120731",
   "metadata": {},
   "source": [
    "## Model Evaluation and Testing\n",
    "\n",
    "1. Trained final model with best parameters from grid search\n",
    "2. Evaluated on validation set using classification metrics\n",
    "3. Processed test data to match training features\n",
    "4. Generated predictions and saved results to CSV\n",
    "5. Converted encoded predictions back to original department names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51acdf93-ece8-4f54-8663-a065b4a2714a",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "### Key Outcomes:\n",
    "- Achieved high cross-validation scores (~99.8%)\n",
    "- Validation and test set predictions showed 100% accuracy\n",
    "- Implemented regularization to mitigate overfitting, but the model is too simple.\n",
    "\n",
    "### Next Steps:\n",
    "1. Real-world deployment testing\n",
    "2. have a front end to test the model & return the predicted dept_name\n",
    "3. handle user not having all columns\n",
    "4. export model\n",
    "5. deploy remotely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c73863-f748-45ab-8ffc-2c3602c637b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd98e0-5f50-45a5-975a-e315e8301b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45bc62e-c086-4ea9-b5c0-16f06eed2226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f8708a-d21b-4396-849d-853be1e15f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
